%\VignetteIndexEntry{High-Dimensional Metrics, Lasso}
%\VignetteDepends{hdm, MASS, glmnet, ggplot2}
%\VignettePackage{hdm}
%\VignetteEngine{knitr}
%\VignetteEncoding{UTF-8}
\documentclass{amsart}
\usepackage{harvard}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[utf8]{inputenc}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}
\newcommand{\Rmethod}[1]{{\textit{#1}}}
\newcommand{\Rfunarg}[1]{{\textit{#1}}}
\newcommand{\R}{{\normalfont\textsf{R }}{}}
\renewcommand{\S}{{\normalfont\textsf{S }}{}}

\title{High-Dimensional Metrics in R: A Tutorial}
\thanks{Version:  \today}
\author{Victor Chernozhukov, Christian Hansen, Martin Spindler}

\begin{document}
%\SweaveOpts{concordance=TRUE}

<<echo=FALSE>>=
library(knitr)
@


\begin{abstract}
High-dimensional Metrics (hdm) is an evolving collection of statistical
methods for estimating and drawing inferences in settings with very many variables. An implementation of some of these methods in the \R language is available in the package \Rpackage{hdm}.
This vignette offers a brief tutorial to
the package and the implemented methods.  \R and the package \Rpackage{hdm} are open-source
software projects and can be freely downloaded from CRAN:
\texttt{http://cran.r-project.org}.
\end{abstract}

\maketitle

\pagestyle{myheadings}
\markboth{\sc High-Dimensional Metrics in \R}{\sc }

\section{Introduction}
Analysis of high-dimensional models, models in which the number of parameters to be estimated is large relative to the sample size, is becoming increasingly important. Such models arise naturally in readily available high-dimensional data which have many measured characteristics available per individual observation as in, for example, large survey data sets, scanner data, and text data.  Such models also arise naturally even in data with a small number of measured characteristics in situations where the exact functional form with which the observed variables enter the model is unknown.  Examples of this scenario include semiparametric models with nonparametric nuisance functions.  More generally, models with many parameters relative to the sample size often arise when attempting to model complex phenomena.

With increasing availability of such data sets in Economics and other fields, new methods for analyzing those data have been developed. The \R package \Rpackage{hdm} contains implementations of recently developed methods with a focus on microeconometric applications and this vignette serves as tutorial. The methods are also useful in other disciplines like Medicine, Biology, Sociology or Psychology to mention a few. The methods which are implemented in this package are distinctive from already available methods in mainly the following: First, the choice of the penalization paramter $\lambda$ in the Lasso regressions is theoretical grounded and feasible. In high-dimensions setting cross-validation is very popular, but lacking a theoretical justification and other proposals are often not feasible. Second, the Lasso regressions allow for heteroscedastic errors. Third, valid inference on low-dimensional (structural) parameters of interest is possible. Examples are inference for selected variables in a high-dimensional regression, a situation which arises in estimation of a treatment effect with very many control variables, or the estimation of a treatment effect in a high-dimensional Instrumental Variable (IV) setting.

In this vignette, we first show how to get started with package. Then we introduce briefly the data sets which are contained in the package and used later for illustration. Next the functions for LASSO and Post-LASSO estimation under heteroscedastic and non-Gaussian errors are presented. They are the core for the further applications. Finally, the



\section{How to get started}
\R is an open source software project and can be freely downloaded from the CRAN
website along with its associated documentation. The \R package \Rpackage{hdm} can be dowloaded from \texttt{cran.r-project.org}. To install the hdm package from \R one simply types,

\vspace{2mm}
\noindent
\texttt{>install.packages("hdm")}
\vspace{2mm}

\noindent
The most current version of the package (development version) is mainatained at R-Forge and can installed by

\vspace{2mm}
\noindent
\texttt{>install.packages("hdm", repos="http://R-Forge.R-project.org")}
\vspace{2mm} 	


\noindent
Provided that your machine has a proper internet connection and you
have write permission in the appropriate system directories,
the installation of the package should proceed automatically.
Once the \texttt{hdm} package is installed, it needs
to be made accessible to the current \R session by the command,
<<results='hide'>>=
library(hdm)
@


Online help is available in two ways.  If you know
precisely the command you are
looking for, e.g. in order to check the datails, try:
<<eval=FALSE>>=
help(package="hdm")
help(lasso)
@
The former command gives an overview over the available commands in the package, and
the latter gives detailed information about a specific command.

More generally one can initiate a web-browser help session with the command,

\vspace{2mm}
\noindent
\texttt{> help.start()}

\vspace{2mm}
\noindent
and navigate as desired.  The browser approach is better adapted to exploratory
inquiries, while the command line approach is better suited to confirmatory ones.

A valuable feature of \R help files is that the examples used to illustrate commands
are executable, so they can be pasted into an \R session, or run as a group with
a command like,

\vspace{2mm}
\noindent
\texttt{>example(lasso)}

\vspace{2mm}
\noindent

\section{Data Sets and Applications}
In this section we describe the data sets which are contained in the package and potential applications.
\subsection{Pension data}
\subsubsection{Pension data}
In the United States 401(k) plans were introduced to to  increase
individual  saving  for  retirement. They allow the individual to
deduct  contributions  from  taxable  income  and  allow  tax-
free accrual of interest on assets held within the plan (within an account). 
Employers  provide  401(k)  plans,  and  employers  may  also
match a certain percentage of an employee's contribution.
Because  401(k)  plans  are  provided  by  employers,  only
workers in firms offering plans are eligible for participation.

The data set can be loaded with

<<results='hide'>>=
data(pension)
@

A description of the variables and further references are given at the help page

<<>>=
help(pension)
@

The sample is drawn from the 1991 Survey of Income and Program Participation (SIPP) and consists of 9,915 observations. The observational units are household reference persons aged 25-64 and spouse if present. Households are included in the sample if at least one person is employed and no one is self-employed. All dollar amounts are in 1991 dollars.
The 1991 SIPP reports household financial data across a
range of asset categories. These data include a variable for
whether a person works for a firm that offers a 401(k) plan.
Households in which a member works for such a
firm are classified  as  eligible  for  a  401(k).  In  addition,  the  survey
also records the amount of 401(k) assets. Households with
a positive 401(k) balance are classified as participants, and eligible  households  with  a  zero  balance  are  considered
nonparticipants. Available measures of wealth in the 1991 SIPP are total wealth, net financial  assets,  and  net  non-401(k) financial  assets.  Net non-401(k)  assets  are  defined  as  the  sum  of  checking  accounts,  U.S.  saving  bonds,  other interest-earning  accounts in  banks  and  other financial institutions,  other  interest-earning assets (such as bonds held personally), stocks and mutual funds less nonmortgage debt, and IRA balances. Net financial  assets  are  net  non-401(k) financial  assets  plus 401(k) balances, and total wealth is net financial assets plus housing  equity  and  the  value  of  business,  property,  and motor vehicles.
\subsubsection{Related Econometric Problems}
Though it is clear that 401(k) plans are widely used as vehicles for retirement saving, their
effect on assets is less clear. The key problem in determin-
ing the effect of participation in 401(k) plans on
accumulated  assets  is  saver  heterogeneity  coupled  with
nonrandom selection into participation states. In particular,
it  is  generally  recognized  that  some  people  have  a  higher
preference for saving than others. Thus, it seems likely that
those individuals with the highest unobserved preference for
saving  would  be  most  likely  to  choose  to  participate  in
tax-advantaged  retirement  savings  plans  and  would  also
have  higher  savings  in  other  assets  than  individuals  with
lower unobserved saving propensity. This implies that con-
ventional estimates that do not allow for saver heterogeneity
and  selection  of  the  participation  state  will  be  biased  upward,  tending  to  overstate  the  actual  savings  effects  of
401(k) and IRA participation.

\subsection{Growth Data}
\subsubsection{Description}
The question what drives Economic Growth, measured in GDP, is a central question of Economics. A famous data set with information about GDP growth for many countries and long period was collected by Barro and Lee. This data set is also provieded in the data set and can be loaded by
<<results='hide'>>=
data(GrowthData)
@
This data sets contains the national growth rates in GDP per capita (Outcome) for many countries with additional covariates. A very important covariate is gdpsh465, which is the initial level of per-capita GDP. For further information we refer to the help page and the references herein.
\subsubsection{Economic Question}
the empirical growth literature is estimating the effect of an initial (lagged) level of
GDP (Gross Domestic Product) per capita on the growth rates of GDP per capita. In
particular, a key prediction from the classical Solow-Swan-Ramsey growth model
is the hypothesis of convergence,which states that poorer countries should typically
grow faster and therefore should tend to catch up with the richer countries. Such
a hypothesis implies that the effect of the initial level of GDP on the growth rate
should be negative. As pointed out in Barro and Sala-i-Martin, this hypothesis
is rejected using a simple bivariate regression of growth rates on the initial level of
GDP. (In this data set, linear regression yields an insignificant positive coefficient
of 0.0013.) In order to reconcile the data and the theory, the literature has focused
on estimating the effect conditional on the pertinent characteristics of countries. Covariates
that describe such characteristics can include variablesmeasuring education
and science policies, strength of market institutions, trade openness, savings rates
and others. The theory then predicts that for countries with similar other characteristics
the effect of the initial level of GDP on the growth rate should be negative. Thus, we are interested in a specification of the form:

\[
\label{GrowthEq}
y_i =\alpha_0  + \alphaa_1 \log  G_i+ \sum_{j=1}^p \beta_j X_{ij} + \varepsilon_i, \]
where $y_i$ is the growth rate of GDP over a specified decade in country $i$, $G_i$ is the
initial level of GDP at the beginning of the specified period, and the $X_{ij}$'s form a
long list of country i's characteristics at the beginning of the specified period. We
are interested in testing the hypothesis of convergence, namely that $\alpha_1 < 0$.
Given that in standard data-sets, such as Barro and Lee data, the number
of covariates $p$ we can condition on is large, at least relative to the sample size $n$,
covariate selection becomes a crucial issue in this analysis. In particular,
previous findings came under severe criticism for relying on ad hoc procedures for
covariate selection. In fact, in some cases, all of the previous findings have been
questioned. Since the number of covariates is high, there is no simple way to
resolve the model selection problemusing only classical tools. Indeed the number of
possible lower-dimensional models is very large, although [16] and [22] attempt to
search over several millions of these models. We suggest $\ell_1$-penalization and post-
$\ell_1$-penalization methods to address this important issue. In Section 8, using these
methods we estimate the growth model (\ref{GrowthEq}) and indeed find rather strong support for
the hypothesis.

\subsection{Instutions and Economic Development}
\subsubsection{Description}
This data set was introduced by paper Acemoglu, Johnson, and Robinson (2001) to analyse the effect of instutitions on economic development. The data is contained in the package and can be accessed with
<<results='hide'>>=
data(AJR)
@

The data set contains GDP, Settler Morality, an index measuring protection against expropration risk and Geographic Information (Latitude and Continent). In total $11$ variables and 64 observations.
\subsubsection{Application}
Estimating the effect of institutions on output is complicated by the clear potential
for simultaneity between institutions and output: specififi cally, better institutions may
lead to higher incomes, but higher incomes may also lead to the development of
better institutions. To help overcome this simultaneity, Acemoglu, Johnson, and
Robinson (2001) use mortality rates for early European settlers as an instrument
for institution quality. The validity of this instrument hinges on the argument that
settlers set up better institutions in places where they are more likely to establish
long-term settlements; that where they are likely to settle for the long term is related
to settler mortality at the time of initial colonization; and that institutions are highly
persistent. The exclusion restriction for the instrumental variable is then motivated
by the argument that GDP, while persistent, is unlikely to be strongly inflfl uenced by
mortality in the previous century, or earlier, except through institutions.

\subsection{Data on Eminent Domain}
\subsubsection{Data Set}
Eminent domain refers to the government's taking of private property,

The data set is loaded into \R by
<<results='hide'>>=
data(EminentDomain)
@


The data set consists of four groups of variables:
\begin{itemize}
\item y: outcome variable, a house price index
\item d: the treatment variable,represents the number of proplaintiff
appellate takings decisions in federal circuit court c and year t
\item x: exogenous control variables that include a dummy variable for whether there were relevant
cases in that circuit-year, the number of takings appellate decisions, and controls for
the distribution of characteristics of federal circuit court judges in a given circuit-year
\item z: instrumental variables, here characteristics of judges serving on federal appellate panels
\end{itemize}
\subsubsection{Economic Application}
 Federal court rulings
that a government seizure was unlawful (pro-plaintiff rulings) thus uphold individual
property rights and make future exercise of eminent domain more diffificult
due to the structure of the US legal system. A more detailed discussion of the
economics of takings law (or eminent domain) and other institutional and econometric
considerations can be found in Belloni, Chen, Chernozhukov, and Hansen
(2012) and Chen and Yeh (2012).
The analysis of the effects of takings law is complicated by the possible endogeneity
between takings law decisions and economic variables: for example, a taking
may be less likely if real estate prices are low and sellers are eager to unload property.
To address the potential endogeneity of takings law, we employ an instrumental
variables strategy based on the identififi cation argument of Chen and Sethi (2010)
and Chen and Yeh (2012) that relies on the random assignment of judges to federal
appellate panels. Because judges are randomly assigned to three-judge panels to
decide appellate cases, the exact identity of the judges and their demographics
are randomly assigned conditional on the distribution of characteristics of federal
circuit court judges in a given circuit-year. Under this random assignment, the
characteristics of judges serving on federal appellate panels can only be related to
property prices through the judges' decisions; thus the judge's characteristics will
plausibly satisfy the instrumental variable exclusion restriction.

\section{LASSO and Post-LASSO Estimation under Heteroscedastic and Non-Gaussian Errors}
\subsection{Estimation}
Short Introduction in LASSO and choice of Lambda\\
Example\\
\subsection{Inference}
Description\\
Example
\section{Instrumental Variable Estimation in a High-Dimensional Setting}
Short Description of Endogeneity\\
Short summary of the three following methods\\
\subsection{Selection on the IVs}
\subsection{Selection on the exogenous variables}
\subsection{Selection on the IVs and exogenous variables}

\section{Estimation of Treatment Effects in a High-Dimensional Setting}

\section{Some Tipps and Tricks}
\section{Conclusion}

An introduction to some of the capabilities of the \R package \texttt{hdm}
package has been given with some examples describing its basic functionality. Inevitably, new applications will
demand new features and, as the project is in its initial phase, unforseen bugs will show up. In either case comments and suggestions of users are highly appreciated. It is intended to update the documentation (including this vignette) and the package periodcally. The most current version of the \R package and its accompanying vignette will be made available at the homepage of the maintainer and \texttt{cran.r-project.org}. See the \R command \texttt{vignette()} for details on how to find
and view vignettes from within \R.
\footnotesize
\bibliographystyle{econometrica}
%\bibliographystyle{aea}
\bibliography{mybib}
\end{document}
